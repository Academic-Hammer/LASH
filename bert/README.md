In order to extract the contextual semantic embedding of the paper text, we leverage the powerful BERT \cite{devlin-etal-2019-bert} model, which has 12-layer, 12-heads Transformer blocks. Despite huge success of the BERT model, it still has limits to represent contextual information in domain specific corpus since it is trained on general corpus. To alleviate this issue in the academic paper domain, we use lots of papers and corresponding field labels to fine-tune the BERT parameters. Specifically, we treat the fine-tuning procedure as the multi-label classification task on the field labels, which has three main steps: (1) For a given paper title and abstract, the contextual semantic embedding $\bm{e}\in \mathbb{R}^{y}$ is obtained by the BERT model; (2) The output feature $\bm{o}=\rm{FNN}(\bm{e})\in \mathbb{R}^{m}$ are obtained based on the fully-connected layer $\rm{FNN}$, where $m$ is the number of the field labels; (3) An activation function $sigmoid(\cdot)$ is used to transform the output feature $o$. Then the binary cross-entropy loss can be obtained by the field label, which will be used to fine-tuning the BERT model parameters. 